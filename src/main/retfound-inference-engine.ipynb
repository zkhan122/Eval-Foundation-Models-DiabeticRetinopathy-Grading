{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yraoDLAe7y5p",
        "outputId": "7f374129-d678-4d62-b38d-5b8be672448e"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kMvplCwHB0eM",
        "outputId": "4955c7fc-d01e-4a53-c660-85d82b1e465d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "System Path Updated.\n",
            "Working Directory: D:\\Zayaan\\D_git\\Eval-Foundation-Models-DiabeticRetinopathy-Grading\\src\n",
            "Dataset Directory Found: D:/Zayaan/D_git/Eval-Foundation-Models-DiabeticRetinopathy-Grading/datasets\n",
            "Contents: ['DeepDRiD', 'IDRID', 'MESSIDOR', 'MFIDDR']\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import os\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "# FOLDER_NAME = \"Eval-Foundation-Models-DiabeticRetinopathy-Grading\"\n",
        "\n",
        "# 1. Define the Main Project Base (The parent of both src and datasets)\n",
        "# PROJECT_BASE_PATH = os.path.join(\"/content/drive/MyDrive\", FOLDER_NAME)\n",
        "\n",
        "# 2. Define the specific sub-paths\n",
        "SRC_PATH = \"D:/Zayaan/D_git/Eval-Foundation-Models-DiabeticRetinopathy-Grading/src\"\n",
        "DATA_DIR = \"D:/Zayaan/D_git/Eval-Foundation-Models-DiabeticRetinopathy-Grading/datasets\"\n",
        "\n",
        "# --- SETUP ---\n",
        "if os.path.exists(SRC_PATH):\n",
        "    # Add 'src' to sys.path for imports\n",
        "    if SRC_PATH not in sys.path:\n",
        "        sys.path.append(SRC_PATH)\n",
        "\n",
        "    # Change working directory to 'src' (keeps relative imports working internally)\n",
        "    os.chdir(SRC_PATH)\n",
        "\n",
        "    print(f\"System Path Updated.\")\n",
        "    print(f\"Working Directory: {os.getcwd()}\")\n",
        "\n",
        "    # Verify Datasets path\n",
        "    if os.path.exists(DATA_DIR):\n",
        "        print(f\"Dataset Directory Found: {DATA_DIR}\")\n",
        "        # List the contents to confirm (IDRiD, DeepDR, etc.)\n",
        "        print(f\"Contents: {os.listdir(DATA_DIR)}\")\n",
        "    else:\n",
        "        print(f\"WARNING: Dataset path not found at: {DATA_DIR}\")\n",
        "else:\n",
        "    print(\"SRC PATH not found\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C1WbPnqcEt8M",
        "outputId": "0cf66090-3301-4304-c11e-3d6006691cf7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu121\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR: Could not find a version that satisfies the requirement torch==2.4.1 (from versions: none)\n",
            "ERROR: No matching distribution found for torch==2.4.1\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: timm==0.9.10 in c:\\users\\zayaa\\miniconda3\\lib\\site-packages (0.9.10)\n",
            "Requirement already satisfied: torch>=1.7 in c:\\users\\zayaa\\miniconda3\\lib\\site-packages (from timm==0.9.10) (2.9.0)\n",
            "Requirement already satisfied: torchvision in c:\\users\\zayaa\\miniconda3\\lib\\site-packages (from timm==0.9.10) (0.24.0)\n",
            "Requirement already satisfied: pyyaml in c:\\users\\zayaa\\miniconda3\\lib\\site-packages (from timm==0.9.10) (6.0.3)\n",
            "Requirement already satisfied: huggingface-hub in c:\\users\\zayaa\\miniconda3\\lib\\site-packages (from timm==0.9.10) (0.36.0)\n",
            "Requirement already satisfied: safetensors in c:\\users\\zayaa\\miniconda3\\lib\\site-packages (from timm==0.9.10) (0.6.2)\n",
            "Requirement already satisfied: filelock in c:\\users\\zayaa\\miniconda3\\lib\\site-packages (from torch>=1.7->timm==0.9.10) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\zayaa\\miniconda3\\lib\\site-packages (from torch>=1.7->timm==0.9.10) (4.15.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\zayaa\\miniconda3\\lib\\site-packages (from torch>=1.7->timm==0.9.10) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\zayaa\\miniconda3\\lib\\site-packages (from torch>=1.7->timm==0.9.10) (3.5)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\zayaa\\miniconda3\\lib\\site-packages (from torch>=1.7->timm==0.9.10) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in c:\\users\\zayaa\\miniconda3\\lib\\site-packages (from torch>=1.7->timm==0.9.10) (2025.9.0)\n",
            "Requirement already satisfied: setuptools in c:\\users\\zayaa\\miniconda3\\lib\\site-packages (from torch>=1.7->timm==0.9.10) (80.9.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\zayaa\\miniconda3\\lib\\site-packages (from sympy>=1.13.3->torch>=1.7->timm==0.9.10) (1.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in c:\\users\\zayaa\\miniconda3\\lib\\site-packages (from huggingface-hub->timm==0.9.10) (25.0)\n",
            "Requirement already satisfied: requests in c:\\users\\zayaa\\miniconda3\\lib\\site-packages (from huggingface-hub->timm==0.9.10) (2.32.5)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\zayaa\\miniconda3\\lib\\site-packages (from huggingface-hub->timm==0.9.10) (4.67.1)\n",
            "Requirement already satisfied: colorama in c:\\users\\zayaa\\miniconda3\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub->timm==0.9.10) (0.4.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\zayaa\\miniconda3\\lib\\site-packages (from jinja2->torch>=1.7->timm==0.9.10) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\zayaa\\miniconda3\\lib\\site-packages (from requests->huggingface-hub->timm==0.9.10) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\zayaa\\miniconda3\\lib\\site-packages (from requests->huggingface-hub->timm==0.9.10) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\zayaa\\miniconda3\\lib\\site-packages (from requests->huggingface-hub->timm==0.9.10) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\zayaa\\miniconda3\\lib\\site-packages (from requests->huggingface-hub->timm==0.9.10) (2025.11.12)\n",
            "Requirement already satisfied: numpy in c:\\users\\zayaa\\miniconda3\\lib\\site-packages (from torchvision->timm==0.9.10) (2.2.6)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\zayaa\\miniconda3\\lib\\site-packages (from torchvision->timm==0.9.10) (12.0.0)\n",
            "Requirement already satisfied: peft in c:\\users\\zayaa\\miniconda3\\lib\\site-packages (0.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in c:\\users\\zayaa\\miniconda3\\lib\\site-packages (from peft) (2.2.6)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\zayaa\\miniconda3\\lib\\site-packages (from peft) (25.0)\n",
            "Requirement already satisfied: psutil in c:\\users\\zayaa\\miniconda3\\lib\\site-packages (from peft) (7.1.3)\n",
            "Requirement already satisfied: pyyaml in c:\\users\\zayaa\\miniconda3\\lib\\site-packages (from peft) (6.0.3)\n",
            "Requirement already satisfied: torch>=1.13.0 in c:\\users\\zayaa\\miniconda3\\lib\\site-packages (from peft) (2.9.0)\n",
            "Requirement already satisfied: transformers in c:\\users\\zayaa\\miniconda3\\lib\\site-packages (from peft) (4.57.3)\n",
            "Requirement already satisfied: tqdm in c:\\users\\zayaa\\miniconda3\\lib\\site-packages (from peft) (4.67.1)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in c:\\users\\zayaa\\miniconda3\\lib\\site-packages (from peft) (1.12.0)\n",
            "Requirement already satisfied: safetensors in c:\\users\\zayaa\\miniconda3\\lib\\site-packages (from peft) (0.6.2)\n",
            "Requirement already satisfied: huggingface_hub>=0.25.0 in c:\\users\\zayaa\\miniconda3\\lib\\site-packages (from peft) (0.36.0)\n",
            "Requirement already satisfied: filelock in c:\\users\\zayaa\\miniconda3\\lib\\site-packages (from huggingface_hub>=0.25.0->peft) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\zayaa\\miniconda3\\lib\\site-packages (from huggingface_hub>=0.25.0->peft) (2025.9.0)\n",
            "Requirement already satisfied: requests in c:\\users\\zayaa\\miniconda3\\lib\\site-packages (from huggingface_hub>=0.25.0->peft) (2.32.5)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\zayaa\\miniconda3\\lib\\site-packages (from huggingface_hub>=0.25.0->peft) (4.15.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\zayaa\\miniconda3\\lib\\site-packages (from torch>=1.13.0->peft) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\zayaa\\miniconda3\\lib\\site-packages (from torch>=1.13.0->peft) (3.5)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\zayaa\\miniconda3\\lib\\site-packages (from torch>=1.13.0->peft) (3.1.6)\n",
            "Requirement already satisfied: setuptools in c:\\users\\zayaa\\miniconda3\\lib\\site-packages (from torch>=1.13.0->peft) (80.9.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\zayaa\\miniconda3\\lib\\site-packages (from sympy>=1.13.3->torch>=1.13.0->peft) (1.3.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\zayaa\\miniconda3\\lib\\site-packages (from tqdm->peft) (0.4.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\zayaa\\miniconda3\\lib\\site-packages (from jinja2->torch>=1.13.0->peft) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\zayaa\\miniconda3\\lib\\site-packages (from requests->huggingface_hub>=0.25.0->peft) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\zayaa\\miniconda3\\lib\\site-packages (from requests->huggingface_hub>=0.25.0->peft) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\zayaa\\miniconda3\\lib\\site-packages (from requests->huggingface_hub>=0.25.0->peft) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\zayaa\\miniconda3\\lib\\site-packages (from requests->huggingface_hub>=0.25.0->peft) (2025.11.12)\n",
            "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\zayaa\\miniconda3\\lib\\site-packages (from transformers->peft) (2025.11.3)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\zayaa\\miniconda3\\lib\\site-packages (from transformers->peft) (0.22.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch==2.4.1 torchvision==0.19.1 torchaudio==2.4.1 --index-url https://download.pytorch.org/whl/cu121\n",
        "!pip install timm==0.9.10\n",
        "!pip install peft"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jRPqmTfrEO59",
        "outputId": "d5b270c6-7728-4ac2-ead9-c6609bd1791d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\zayaa\\miniconda3\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "from models.RETFound_MAE import models_vit\n",
        "from models.RETFound_MAE.util import pos_embed\n",
        "from timm.models.layers import trunc_normal_\n",
        "import torchvision\n",
        "from data_processing.dataloader import load_idrid_grading_labels\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from data_processing.dataset import CombinedDRDataSet\n",
        "from utilities.utils import identity_transform, show_images, train_one_epoch, validate\n",
        "from utilities.create_split import extract_for_split\n",
        "from hparams.hparams import NUM_CLASSES,BATCH_SIZE, NUM_EPOCHS, LEARNING_RATE, NUM_WORKERS, DEVICE\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sHMhPIVI7z2k"
      },
      "source": [
        "Data initialization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 1744 total images.\n",
            "Calculating test size as 20% (348 images).\n",
            "Destination folder 'D:/Zayaan/D_git/Eval-Foundation-Models-DiabeticRetinopathy-Grading/datasets/MESSIDOR\\messidor-2\\messidor-2\\test' ensured.\n",
            "Successfully copied 348 images to 'D:/Zayaan/D_git/Eval-Foundation-Models-DiabeticRetinopathy-Grading/datasets/MESSIDOR\\messidor-2\\messidor-2\\test'.\n",
            "Starting Data Split for MESSIDOR...\n",
            "Found 1396 total images.\n",
            "Calculating test size as 20% (279 images).\n",
            "Destination folder 'D:/Zayaan/D_git/Eval-Foundation-Models-DiabeticRetinopathy-Grading/datasets/MESSIDOR\\messidor-2\\messidor-2\\test' ensured.\n",
            "Successfully copied 279 images to 'D:/Zayaan/D_git/Eval-Foundation-Models-DiabeticRetinopathy-Grading/datasets/MESSIDOR\\messidor-2\\messidor-2\\test'.\n",
            "MESSIDOR split complete.\n"
          ]
        }
      ],
      "source": [
        "root_directories = {\n",
        "    \"DEEPDRID\": f\"{DATA_DIR}/DeepDRiD\",\n",
        "    \"IDRID\": f\"{DATA_DIR}/IDRID\",\n",
        "    \"MESSIDOR\": f\"{DATA_DIR}/MESSIDOR\",\n",
        "    \"MFIDDR\": f\"{DATA_DIR}/MFIDDR\", # only use for test if necessary\n",
        "}\n",
        "\n",
        "MESSIDOR_BASE_PATH = root_directories[\"MESSIDOR\"]\n",
        "\n",
        "SOURCE_DIR = os.path.join(MESSIDOR_BASE_PATH, \"messidor-2\", \"messidor-2\", \"preprocess\") \n",
        "DESTINATION_DIR = os.path.join(MESSIDOR_BASE_PATH, \"messidor-2\", \"messidor-2\", \"test\")\n",
        "\n",
        "# SOURCE_DIR = \"datasets/MESSIDOR/messidor-2/messidor-2/preprocess\"\n",
        "# DESTINATION_DIR = \"datasets/MESSIDOR/messidor-2/messidor-2/test\"\n",
        "\n",
        "TEST_FRACTION = 0.2 \n",
        "\n",
        "extract_for_split(SOURCE_DIR, DESTINATION_DIR, TEST_FRACTION)\n",
        "\n",
        "\n",
        "print(\"Starting Data Split for MESSIDOR...\")\n",
        "extract_for_split(SOURCE_DIR, DESTINATION_DIR, TEST_FRACTION)\n",
        "print(\"MESSIDOR split complete.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_z7u1aEK7zgn",
        "outputId": "9d25cee1-c227-42d1-db18-25376b6aa372"
      },
      "outputs": [],
      "source": [
        "train_transformations = transforms.Compose([\n",
        "    transforms.Resize((192, 192)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                        std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "test_transformations = transforms.Compose([\n",
        "    transforms.Resize((192, 192)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                        std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "train_dataset = CombinedDRDataSet(root_directories=root_directories, split=\"train\", img_transform=train_transformations, label_transform=identity_transform)\n",
        "# test_dataset = CombinedDRDataSet(root_directories=root_directories, split=\"test\", img_transform=test_transformations)\n",
        "\n",
        "\n",
        "print(\"Labels\", train_dataset.get_labels())\n",
        "\n",
        "# loading csv_paths (removed MFIDDR as too little samples , but possibly can use in testing)\n",
        "train_csv_paths = {\n",
        "    \"IDRID\": f\"{root_directories['IDRID']}/B-Disease-Grading/Disease-Grading/2-Groundtruths/IDRiD_Disease_Grading_Training_Labels.csv\",\n",
        "    \"DEEPDRID\": f\"{root_directories['DEEPDRID']}/regular_fundus_images/regular-fundus-training/regular-fundus-training.csv\",\n",
        "    \"MESSIDOR\": f\"{root_directories['MESSIDOR']}/messidor_data.csv\",\n",
        "}\n",
        "\n",
        "# test_csv_paths = {\n",
        "#     \"IDRID\": f\"{root_directories['IDRID']}/B-Disease-Grading/Disease-Grading/2-Groundtruths/IDRiD_Disease_Grading_Testing_Labels.csv\",\n",
        "#     \"DEEPDRID\": f\"{root_directories['DEEPDRID']}/regular_fundus_images/regular-fundus-validation/regular-fundus-validation.csv\",\n",
        "#     \"MFIDDR\": f\"{root_directories['MFIDDR']}/sample/test_fourpic_label.csv\"\n",
        "# }\n",
        "\n",
        "train_labels = train_dataset.load_labels_from_csv(train_csv_paths)\n",
        "# train_dataset.load_labels_from_csv(test_csv_paths)\n",
        "\n",
        "\n",
        "# print(\"TRAIN DATASET LENGTH:\", train_dataset.__len__()) # 11/11/25 len is 0 for both hence there is error in data preprocessing\n",
        "# print(\"TEST DATASET LENGTH:\", test_dataset.__len__())\n",
        "\n",
        "\n",
        "\n",
        "# printing dataset statistics\n",
        "print(\"Training Set Statistics:\")\n",
        "print(train_dataset.get_dataset_statistics())\n",
        "\n",
        "\n",
        "# print(\"\\nTest Set Statistics:\")\n",
        "# print(test_dataset.get_dataset_statistics())\n",
        "\n",
        "# --- FIX: Set num_workers=0 to prevent freezing on Drive ---\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=4,\n",
        "    shuffle=True,\n",
        "    num_workers=0\n",
        ")\n",
        "\n",
        "print(train_loader.batch_size)\n",
        "\n",
        "# test_loader = DataLoader(\n",
        "#     test_dataset,\n",
        "#     batch_size=32,\n",
        "#     shuffle=False,\n",
        "#     num_workers=4\n",
        "# )\n",
        "\n",
        "\n",
        "print(train_loader)\n",
        "\n",
        "# checking inside dataset\n",
        "# for idx in range(len(train_dataset)):\n",
        "#     image, label, metadata = train_dataset[idx]\n",
        "#     print(f\"Image: {image} -- Label: {label} -- Metadata: {metadata}\")\n",
        "\n",
        "dataset_length = len(train_dataset)\n",
        "print(\"total samples: \", dataset_length)\n",
        "\n",
        "# show_images(train_dataset, train_labels, num_images=60, start_idx=0)\n",
        "\n",
        "# mfiddr -> idrid -> deepdrid\n",
        "# 413 idrid\n",
        "# 1661 total\n",
        "\n",
        "print(f\"\\nTrain batches: {len(train_loader)}\")\n",
        "# print(f\"Test batches: {len(test_loader)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wPpU7-Xa8LEP"
      },
      "source": [
        "RETFound model intialization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M4MhJ9QLDDJr"
      },
      "outputs": [],
      "source": [
        "from peft import get_peft_model, LoraConfig, TaskType\n",
        "\n",
        "# show_images(train_dataset, train_labels, num_images=60, start_idx=0)\n",
        "\n",
        "# mfiddr -> idrid -> deepdrid\n",
        "# 413 idrid\n",
        "# 1661 total\n",
        "\n",
        "print(f\"\\nTrain batches: {len(train_loader)}\")\n",
        "# print(f\"Test batches: {len(test_loader)}\")\n",
        "\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Initializing RETFound Model\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "model = models_vit.__dict__[\"vit_large_patch16\"](\n",
        "    num_classes=NUM_CLASSES,\n",
        "    drop_path_rate=0.2,\n",
        "    global_pool=True\n",
        ")\n",
        "\n",
        "checkpoint_path = f\"{SRC_PATH}/models/RETFound_MAE/weights/RETFound_cfp_weights.pth\"\n",
        "print(f\"Loading pretrained weights from: {checkpoint_path}\")\n",
        "\n",
        "checkpoint = torch.load(checkpoint_path, map_location=\"cpu\", weights_only=False)\n",
        "checkpoint_model = checkpoint[\"model\"]\n",
        "state_dict = model.state_dict()\n",
        "\n",
        "# removing head weights (as they dont match number of classes for severity grading 0-4)\n",
        "for k in [\"head.weight\", \"head.bias\"]:\n",
        "    if k in checkpoint_model and checkpoint_model[k].shape != state_dict[k].shape:\n",
        "        print(f\"Removing key {k} from pretrained checkpoint (shape mismatch)\")\n",
        "        del checkpoint_model[k]\n",
        "\n",
        "pos_embed.interpolate_pos_embed(model, checkpoint_model)\n",
        "\n",
        "# Loading the weights\n",
        "msg = model.load_state_dict(checkpoint_model, strict=False)\n",
        "print(f\"Missing keys: {msg.missing_keys}\")\n",
        "print(f\"Unexpected keys: {msg.unexpected_keys}\")\n",
        "\n",
        "# verifying we're only missing the classification head\n",
        "assert set(msg.missing_keys) == {\"head.weight\", \"head.bias\", \"fc_norm.weight\", \"fc_norm.bias\"}\n",
        "\n",
        "# initializing the new classification head\n",
        "trunc_normal_(model.head.weight, std=2e-5)\n",
        "\n",
        "print(\"\\nWrapping model with LoRA adapters...\")\n",
        "peft_config = LoraConfig(\n",
        "    r=16,                  # rank\n",
        "    lora_alpha=16,\n",
        "    target_modules=[\"qkv\"], # target attention layers in ViT\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\",\n",
        "    modules_to_save=[\"head\", \"fc_norm\"] # keeping classification head and norm trainable\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, peft_config)\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "model = model.to(DEVICE)\n",
        "\n",
        "# param count\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"\\nTotal parameters: {total_params:,}\")\n",
        "print(f\"Trainable parameters: {trainable_params:,}\")\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=0.05)\n",
        "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=NUM_EPOCHS)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r0Ao80tR7XkR"
      },
      "source": [
        "Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "li3jbJ4A7Wy5"
      },
      "outputs": [],
      "source": [
        "# Training\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Starting Training\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "best_val_acc = 0.0\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    print(f\"\\nEpoch {epoch+1}/{NUM_EPOCHS}\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    train_loss, train_acc = train_one_epoch(\n",
        "        model, train_loader, criterion, optimizer, DEVICE, epoch\n",
        "    )\n",
        "\n",
        "    # Validate\n",
        "    # val_loss, val_acc = validate(model, test_loader, criterion)\n",
        "\n",
        "    # lr update\n",
        "    scheduler.step()\n",
        "\n",
        "    print(f\"\\nEpoch {epoch+1} Summary:\")\n",
        "    print(f\"  Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%\")\n",
        "    # print(f\"  Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}% \")\n",
        "    print(f\"  Learning Rate: {optimizer.param_groups[0]['lr']:.6f}\")\n",
        "\n",
        "    # Saving best model\n",
        "    # if val_acc > best_val_acc:\n",
        "    #     best_val_acc = val_acc\n",
        "    #     print(f\"  âœ“ New best validation accuracy: {val_acc:.2f}%\")\n",
        "    #     torch.save({\n",
        "    #         'epoch': epoch,\n",
        "    #         'model_state_dict': model.state_dict(),\n",
        "    #         'optimizer_state_dict': optimizer.state_dict(),\n",
        "    #         'val_acc': val_acc,\n",
        "    #         'val_loss': val_loss,\n",
        "    #     }, 'best_retfound_model.pth')\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Training Complete!\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Best Validation Accuracy: {best_val_acc:.2f}%\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
